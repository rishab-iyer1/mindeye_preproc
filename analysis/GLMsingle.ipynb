{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLMsingle: single-trial estimation in rapid event-related design\n",
    "\n",
    "This notebook is my attempt at applying the new GLMsingle toolbox to the SVD dataset. I am starting with the tutorial provided on the GLMsingle GitHub and modifying it for the SVD dataset. (LM)\n",
    "https://htmlpreview.github.io/?https://github.com/kendrickkay/GLMsingle/blob/main/examples/example1.html   \n",
    "\n",
    "By default, the tool implements a set of optimizations that improve upon generic GLM approaches by: (1) identifying an optimal hemodynamic response function (HRF) at each voxel, (2) deriving a set of useful GLM nuisance regressors via \"GLMdenoise\" and picking an optimal number to include in the final GLM, and (3) applying a custom amount of ridge regularization at each voxel using an efficient technique called \"fracridge\". The output of GLMsingle are GLM betas reflecting the estimated percent signal change in each voxel in response to each experimental stimulus or condition being modeled.\n",
    "\n",
    "Example 1 contains a full walkthrough of the process of loading an example dataset and design matrix, estimating neural responses using GLMsingle, estimating the reliability of responses at each voxel, and comparing those achieved via GLMsingle to those achieved using a baseline GLM. After loading and visualizing formatted fMRI time-series and their corresponding design matrices, we will describe the default behavior of GLMsingle and show how to modify hyperparameters if the user desires. Throughout the notebook we will highlight important metrics and outputs using figures, print statements, and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note about conda environments\n",
    "\n",
    "When running this notebook, make sure you have activated the mindeye conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What subject and session do you want to run? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 'sub-005'\n",
    "session = 'all'  # 'ses-xx', 'all'\n",
    "task='task-C_'  # '' (empty string) normally, task-A_ or similar otherwise\n",
    "func_task_name = 'C'  # 'study' or 'A'; used to search for functional run in bids format\n",
    "resample_voxel_size = False\n",
    "\n",
    "# settings for multiple sessions\n",
    "if session == \"all\":\n",
    "    ses_list = [\"ses-01\", \"ses-02\", \"ses-03\"]  # list of actual session IDs\n",
    "    design_ses_list = [\"ses-01\", \"ses-02\", \"ses-03\"]  # list of session IDs to search for design matrix\n",
    "else:\n",
    "    ses_list = [session]\n",
    "    design_ses_list = [session]\n",
    "    \n",
    "ref_session = None  # None or 'ses-xx'; use the T1 from the reference session; use this only if the current session doesn't have a T1\n",
    "if ref_session is not None: \n",
    "    assert session == \"all\"\n",
    "# if ref_session is None: ref_session = session\n",
    "\n",
    "if sub=='sub-001' and session == 'ses-01':\n",
    "    n_runs=16\n",
    "elif sub=='sub-002' and session == 'ses-01':\n",
    "    n_runs=9\n",
    "elif (sub=='sub-001' and session in ('ses-05')) or (sub=='sub-002' and session in ('ses-02')) or (sub=='sub-004' and session in ('ses-01')):\n",
    "    n_runs=5\n",
    "elif sub=='sub-004' and session  == 'ses-02':\n",
    "    n_runs = 12\n",
    "elif sub=='sub-004' and session == 'all':\n",
    "    n_runs = 17\n",
    "    runs_per_session = {\n",
    "    \"ses-01\": 5,\n",
    "    \"ses-02\": 12\n",
    "    }\n",
    "elif sub=='sub-005' and session in ('ses-01', 'ses-02', 'ses-03'):\n",
    "    n_runs=11\n",
    "elif sub=='sub-005' and session == 'all' and ses_list == [\"ses-01\", \"ses-02\"]:\n",
    "    n_runs=22\n",
    "    runs_per_session = {\n",
    "    \"ses-01\": 11,\n",
    "    \"ses-02\": 11\n",
    "    }\n",
    "elif sub=='sub-005' and session == 'all' and ses_list == [\"ses-01\", \"ses-02\", \"ses-03\"]:\n",
    "    n_runs=33\n",
    "    runs_per_session = {\n",
    "    \"ses-01\": 11,\n",
    "    \"ses-02\": 11,\n",
    "    \"ses-03\": 11\n",
    "    }\n",
    "else:\n",
    "    raise Exception(\"undefined subject and/or session\")\n",
    "runs_per_session = {session: n_runs} if not session == \"all\" else runs_per_session\n",
    "if session == \"all\": assert runs_per_session is not None\n",
    "print(n_runs, \"functional runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do you want to re-calculate the design matrix? If the design matrix already exists, set this to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_design = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do you want to intersect brain masks from each run to create an average mask? If avg_mask already exists, set this to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_avg_mask = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to load the raw 4D data and run the time-intensive NiftiMasker? \n",
    "If the 2D data already exist, set this to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_epi = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to resample the voxel size of the fMRI data? If the upsampled data already exist, set this to 0. If you do not want to resample voxels (resample_voxel_size is False), this will have no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_resample_voxel = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you need to upsample the fMRI data? If the upsampled data already exists, set this to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_upsampling = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to run GLMsingle? If you want to load existing GLMsingle outputs, set this to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_glmsingle = 1\n",
    "run_glmbaseline = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to run reliability calculations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_reliability = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nibabel import imagestats\n",
    "import nilearn\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import intersect_masks\n",
    "from nilearn import image\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.plotting import plot_anat\n",
    "from nilearn.plotting import plot_epi\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import join, exists, split\n",
    "import sys\n",
    "import time\n",
    "import urllib.request\n",
    "import copy\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings('ignore')\n",
    "import h5py\n",
    "\n",
    "sys.path.insert(0, '/mnt/cup/labs/norman/rsiyer/rtmindeye/code/analysis')  # used if running this as a notebook or script from a directory that doesn't include tseriesinterp\n",
    "from tseriesinterp import tseriesinterp, reshape2D\n",
    "\n",
    "from utils import get_session_label, process_design, create_design_matrix, compare_mask_epi_dims, mask_info, fit_save_bold, resample, applyxfm, apply_thresh, load_design_files\n",
    "\n",
    "# !pip install git+https://github.com/cvnlab/GLMsingle.git\n",
    "# import glmsingle\n",
    "from glmsingle.glmsingle import GLM_single\n",
    "\n",
    "%matplotlib inline\n",
    "# %autosave 5\n",
    "#sns.set(style = 'white', context='poster', rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "if resample_voxel_size:\n",
    "    try: \n",
    "        os.system(\"flirt -version\")\n",
    "    except:\n",
    "        raise Exception(\"make sure you run \\\"module load fsl\\\" in order to resample voxel size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths and load settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_label = get_session_label(ses_list)\n",
    "\n",
    "example = 0\n",
    "stimdur = 3\n",
    "if sub == 'sub-001' and session_label == 'ses-01':\n",
    "    tr_orig = 1.5\n",
    "    n_trs = 288\n",
    "elif (sub == 'sub-001' and session_label == 'ses-05') or \\\n",
    "     (sub == 'sub-002' and session_label == 'ses-02') or \\\n",
    "     (sub == 'sub-004' and session_label in ('ses-01', 'ses-02', 'ses-01-02')) or \\\n",
    "     (sub == 'sub-005' and session_label in ('ses-01', 'ses-02', 'ses-03', 'ses-01-02', 'ses-01-03')):\n",
    "    if func_task_name == 'A':\n",
    "        tr_orig = 2\n",
    "        n_trs = 280\n",
    "    elif func_task_name in ['B', 'C']:\n",
    "        tr_orig = 1.5\n",
    "        n_trs = 288\n",
    "    else:\n",
    "        raise Exception(\"invalid task\")\n",
    "else:\n",
    "    raise Exception(\"invalid subject and/or session\")\n",
    "\n",
    "tr = 1  # after upsampling\n",
    "\n",
    "if resample_voxel_size:\n",
    "    resampled_vox_size = 3.5\n",
    "    resample_method = \"trilinear\"  # {trilinear,nearestneighbour,sinc,spline}, credit: https://johnmuschelli.com/fslr/reference/flirt.help.html\n",
    "    \n",
    "    # file name helper variables\n",
    "    vox_dim_str = str(resampled_vox_size).replace('.', '_')  # in case the voxel size has a decimal, replace with an underscore\n",
    "    resampled_suffix = f\"resampled_{vox_dim_str}mm_{resample_method}\"\n",
    "\n",
    "print(f\"original TR: {tr_orig}, total TRs: {n_trs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bids directory\n",
    "# if sub == 'sub-001' and session == 'ses-01':\n",
    "#     homedir = f'/jukebox/norman/rsiyer/rtmindeye/data_sub-001_sub_002_ses-01/bids'\n",
    "# else:\n",
    "if sub in ('sub-004', 'sub-005'):\n",
    "    homedir = f'/jukebox/norman/rsiyer/rtmindeye/data_{sub}/bids'\n",
    "else:\n",
    "    homedir = f'/jukebox/norman/rsiyer/rtmindeye/data_{sub}_{session}/bids'\n",
    "if not os.path.exists(homedir):\n",
    "    raise FileNotFoundError(f\"Folder '{homedir}' does not exist.\")\n",
    "\n",
    "# datadir is fmriprep folder for this subject, outputdir is glmsingle derivatives\n",
    "datadir = join(homedir,'derivatives','fmriprep',sub)\n",
    "maskdir = join(homedir,'derivatives','masks', sub)\n",
    "designdir = join(homedir, '../design')\n",
    "outputdir = join(homedir,'derivatives',f'glmsingle_{session_label}_task-{func_task_name}')\n",
    "\n",
    "# if session == \"all\":\n",
    "#     suffix = [s for s in ses_list]\n",
    "#     session_suffix = f\"_{len(ses_list)}ses\"\n",
    "#     for s in suffix:\n",
    "#         session_suffix += f\"_{s}\"\n",
    "#     outputdir += session_suffix\n",
    "\n",
    "if resample_voxel_size == True:\n",
    "    outputdir += f\"_{resampled_suffix}\"\n",
    "\n",
    "    resampled_dir = f\"{outputdir}/resampled_data\"\n",
    "    os.makedirs(resampled_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{resampled_dir}/func\", exist_ok=True)\n",
    "    print(f'directory to save resampled data:\\n\\t{resampled_dir}\\n', flush=True)\n",
    "\n",
    "print(outputdir)\n",
    "# assert os.path.exists(outputdir)\n",
    "# func_dir = join(datadir,f'{session}','func/')\n",
    "defaced_dir = join(homedir,'derivatives','deface/')\n",
    "savedir = f'{outputdir}/{sub}/'  # where to save voxel reliabilities\n",
    "\n",
    "path = os.path.join(outputdir,sub)\n",
    "os.makedirs(path,exist_ok=True)\n",
    "os.makedirs(maskdir,exist_ok=True)\n",
    "\n",
    "print(f'directory to load preprocessed data from:\\n\\t{datadir}\\n', flush=True)\n",
    "print(f'directory to save outputs:\\n\\t{path}\\n', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran into an issue because of my small voxels, I have more than 1,000,000 \"voxels\" in my brain images (including voxels outside the brain).\n",
    "\n",
    "I'm going to use NiftiMasker to apply the average brain mask to data, then instead of passing 4D (XYZT) images to data array, pass 2D (units x time) to data array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load this subject's design matrix\n",
    "design is in the format design[run] rows are timepoints, columns are distinct images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, starts, images, is_new_run, unique_images, len_unique_images = load_design_files(\n",
    "    sub=sub,\n",
    "    session=session,\n",
    "    func_task_name=func_task_name,\n",
    "    designdir=designdir,\n",
    "    design_ses_list=design_ses_list\n",
    ")\n",
    "\n",
    "# assert len(images[images!=\"blank.jpg\"]) == 1008\n",
    "# assert len(np.unique(images[images!=\"blank.jpg\"])) == 708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix_dir = f\"{designdir}/{sub}_{session_label}_{task}design_matrix.npy\"\n",
    "if make_design == 1:\n",
    "    # Process each session separately\n",
    "    design_list = []\n",
    "    for ses in design_ses_list:\n",
    "        filename = f\"{designdir}/csv/{sub}_{ses}.csv\"\n",
    "        print(f\"Processing design matrix for: {filename}\")\n",
    "        \n",
    "        # Get data for this session\n",
    "        data, starts, images, is_new_run, image_names = process_design(filename)\n",
    "        \n",
    "        # Create design matrix for this session\n",
    "        design_ses = create_design_matrix(\n",
    "            images=images,\n",
    "            starts=starts, \n",
    "            is_new_run=is_new_run,\n",
    "            unique_images=unique_images,\n",
    "            n_runs=runs_per_session.get(ses, 0),\n",
    "            n_trs=n_trs,\n",
    "            len_unique_images=len_unique_images\n",
    "        )\n",
    "        \n",
    "        # print(design_ses)\n",
    "        design_list.extend(design_ses)\n",
    "    # Save concatenated design matrix\n",
    "    design = np.array(design_list)\n",
    "    np.save(design_matrix_dir, design)\n",
    "    print(\"saved design matrix to\", design_matrix_dir)\n",
    "    print(design.shape)\n",
    "\n",
    "else:\n",
    "    design = np.load(design_matrix_dir)\n",
    "    \n",
    "# Find columns with zero sum across all dimensions\n",
    "zero_cols = np.sum(design, axis=(0,1)) == 0\n",
    "cols_to_delete = np.where(zero_cols)[0]\n",
    "\n",
    "print(f'shape of design matrix: {design.shape}')\n",
    "print(f'deleting conditions {cols_to_delete} because there are no occurrences.')\n",
    "\n",
    "# Delete all zero columns at once\n",
    "design = np.delete(design, cols_to_delete, axis=2)\n",
    "\n",
    "print(f'shape of design matrix: {design.shape}')\n",
    "design = [d for d in design]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(str(images[5])==unique_images)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [print(im.split('/')[-1]) for im in unique_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.array(design)\n",
    "# test.shape, np.sum(np.sum(test, axis=0), axis=0).shape  # np.sum(np.sum(test, axis=0), axis=0), np.where(np.sum(np.sum(test, axis=0), axis=0) == 0)\n",
    "# design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # what are the first 10 images shown?\n",
    "# print(images[:10])\n",
    "\n",
    "# # what condition in the design matrix do these correspond with?\n",
    "# first10 = [np.where(im == unique_images)[0].item() for im in images[:10]]\n",
    "# print(first10)\n",
    "\n",
    "# # at what timepoint did these appear?\n",
    "# np.where(test[:, :, first10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(np.where(test[:,:,:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot design matrix for a run\n",
    "run=0\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(design[run],interpolation='none')\n",
    "plt.title(' design matrix from run %i' %(run+1), fontsize=16)\n",
    "plt.xlabel('conditions',fontsize=16)\n",
    "plt.ylabel('time (TR)',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find conditions that are repeated in the same run\n",
    "# for i in range(16):\n",
    "#     print(np.where(np.sum(design[i], axis=0) > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find conditions that are repeated across all runs\n",
    "# np.where((np.sum(np.sum(np.array(design), axis=1), axis=0) > 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a brain mask that intersects each run's brain mask from fMRIPrep\n",
    "\n",
    "From GLMSingle FAQ: The safest approach is to completely zero out the data for a voxel that does not have full data for all of the runs that you are analyzing with GLMsingle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_avg_mask == 1:\n",
    "    mask_imgs = []\n",
    "    for ses in ses_list:\n",
    "        func_dir = join(datadir,f'{ses}','func')\n",
    "        print(func_dir)\n",
    "        for run in range(1, runs_per_session.get(f'{ses}', 0) + 1):\n",
    "            mask_name = f\"{func_dir}/{sub}_{ses}_task-{func_task_name}_run-{run:02}_space-T1w_desc-brain_mask.nii.gz\"\n",
    "            mask_imgs.append(mask_name)\n",
    "\n",
    "    func_masks = [nilearn.image.load_img(mask) for mask in mask_imgs]\n",
    "    avg_mask = intersect_masks(func_masks, threshold=0.5, connected=True)\n",
    "\n",
    "    # mask_imgs=[]\n",
    "    # for run in range(1,n_runs+1):\n",
    "    #     mask_name = f\"{func_dir}/{sub}_{session}_task-{func_task_name}_run-{run:02}_space-T1w_desc-brain_mask.nii.gz\"\n",
    "    #     mask_imgs.append(mask_name)\n",
    "    \n",
    "    # avg_mask=intersect_masks(mask_imgs, threshold=0.5, connected=True)\n",
    "\n",
    "    # Save the mask\n",
    "    dimsize=avg_mask.header.get_zooms()\n",
    "    affine_mat = avg_mask.affine\n",
    "    hdr = avg_mask.header  # get a handle for the .nii file's header\n",
    "    hdr.set_zooms((dimsize[0], dimsize[1], dimsize[2]))\n",
    "    output_name = f\"{maskdir}/{sub}_{session_label}_{task}brain.nii.gz\"\n",
    "    print('Save average mask:', output_name)\n",
    "    nib.save(avg_mask, output_name)\n",
    "    print('')\n",
    "    \n",
    "elif make_avg_mask == 0:\n",
    "    #Load brain mask\n",
    "    file_in = maskdir + '/%s_%s_%sbrain.nii.gz' % (sub, session, task)\n",
    "    print('Loading average brain mask:', file_in)\n",
    "    print('')\n",
    "    avg_mask=nib.load(file_in)\n",
    "\n",
    "if ref_session is not None:\n",
    "    t1_file = f\"{defaced_dir}{sub}_{ref_session}_T1w_defaced.nii.gz\"\n",
    "else:\n",
    "    # verify all defaced T1 images have the same dimensions and affine\n",
    "    t1_file = f\"{defaced_dir}{sub}_{ses_list[0]}_T1w_defaced.nii.gz\"\n",
    "    t1_img = image.load_img(t1_file)\n",
    "    dimsize, affine_mat, brain, xyz = mask_info(avg_mask, t1_img)\n",
    "    for ses in ses_list:\n",
    "        t1_file = f\"{defaced_dir}{sub}_{ses}_T1w_defaced.nii.gz\"\n",
    "        t1_img = image.load_img(t1_file)\n",
    "        dimsize_ref, affine_mat_ref, brain_ref, xyz_ref = mask_info(avg_mask, t1_img)\n",
    "        assert np.all(dimsize == dimsize_ref) and np.all(affine_mat == affine_mat_ref) and np.all(brain == brain_ref) and np.all(xyz == xyz_ref)\n",
    "    print(\"all defaced T1 images have the same dimensions and affine!\")\n",
    "\n",
    "t1_img = image.load_img(t1_file)\n",
    "\n",
    "dimsize, affine_mat, brain, xyz = mask_info(avg_mask, t1_img, plot=True)\n",
    "\n",
    "if resample_voxel_size:\n",
    "    assert dimsize[0] == dimsize[1] and dimsize[1] == dimsize[2], \"voxels are not isometric\"\n",
    "    assert dimsize[2] != resampled_vox_size, f\"attempting to resample from {dimsize[2]}mm to {resampled_vox_size}mm voxels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply mask to each run of functional data\n",
    "Data will go from 4D (x,y,z,t) to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for ses in ses_list:\n",
    "    epi_file=f\"{datadir}/{ses}/func/{sub}_{ses}_task-{func_task_name}_run-01_space-T1w_desc-preproc_bold.nii.gz\"\n",
    "    compare_mask_epi_dims(epi_file, avg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variables that will contain bold time-series and design matrices from each run\n",
    "data = []\n",
    "TR_run = []\n",
    "\n",
    "for ses in ses_list:\n",
    "    for run in range(runs_per_session.get(ses, 0)):\n",
    "        filename = f\"{outputdir}/{sub}/{sub}_{ses}_task-{func_task_name}_run-{(run+1):02}_2D_bold.npy\"\n",
    "\n",
    "        if load_epi == 1:\n",
    "            epi_file = f\"{datadir}/{ses}/func/{sub}_{ses}_task-{func_task_name}_run-{(run+1):02}_space-T1w_desc-preproc_bold.nii.gz\"\n",
    "            epi_mask_data = fit_save_bold(epi_file, avg_mask, filename, run)\n",
    "        elif load_epi == 0:\n",
    "            print(f'\\n*** Loading data from {filename} ***\\n')\n",
    "            epi_mask_data = np.load(filename)\n",
    "\n",
    "        # Add this run's data to data variable\n",
    "        data.append(epi_mask_data)\n",
    "        TR_run.append(epi_mask_data.shape[1])\n",
    "\n",
    "        print('BOLD data shape:', epi_mask_data.shape)\n",
    "        print('TRs in this run:', epi_mask_data.shape[1])\n",
    "\n",
    "\n",
    "print('')\n",
    "print('number of runs in BOLD data:', len(data))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Resample voxel size \n",
    "We will optionally resample the voxel size of the functional runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resample_voxel_size:\n",
    "    # set variables that are needed regardless of whether we want to run the voxel resampling (run_resample_voxel) or load in resampled data\n",
    "\n",
    "    # mask paths\n",
    "    m = '%s_%s_%sbrain' % (sub, session, task)\n",
    "    mask_in_name = f\"{maskdir}/{m}.nii.gz\"\n",
    "    mask_out_name = f\"{resampled_dir}/{m}_{resampled_suffix}.nii.gz\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resample_voxel_size:  \n",
    "    # even if not performing resampling here (run_resample_voxel == False), do verification since this flag is True\n",
    "    # check affines of all the bold runs we just loaded in \n",
    "    \n",
    "    d0_affine = nilearn.masking.unmask(data[0].T, avg_mask).affine\n",
    "    print(d0_affine)\n",
    "    for d in range(len(data)):\n",
    "        assert np.all(nilearn.masking.unmask(data[d].T, avg_mask).affine == d0_affine)\n",
    "        boldref_filename = os.path.join(datadir, session, f'func/{sub}_{session}_task-{func_task_name}_run-{d+1:02}_space-T1w_boldref.nii.gz')\n",
    "        assert np.all(nib.load(boldref_filename).affine == d0_affine)\n",
    "        \n",
    "    print(\"all the BOLD runs and boldref affines match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if resample_voxel_size:\n",
    "    # now that we've checked that the bold and boldrefs have the same affines (i.e. they're in the same space),\n",
    "    # we just need to pick one of them and resample it to our desired voxel size using flirt, then use that as a \n",
    "    # reference image to resample all the bold runs. then proceed to the rest of the script as normal\n",
    "    \n",
    "    omat_name = f\"{resampled_dir}/boldref_omat\"\n",
    "    ref_name = f\"{resampled_dir}/boldref_resampled\"\n",
    "    print(f\"path to save transformation matrix (should be the identity matrix): {omat_name}\")\n",
    "    print(f\"path to save resampled boldref image: {ref_name}\")\n",
    "\n",
    "    resample(boldref_filename, boldref_filename, resampled_vox_size, omat_name, output=ref_name)\n",
    "\n",
    "    # assert that the resampled output matrix is the identity matrix; this should be true because we are not aligning to a different space, i.e. applying an affine/nonlinear transform, just changing the voxel size\n",
    "    assert np.all(np.loadtxt(omat_name) == np.eye(4))\n",
    "    assert exists(f\"{ref_name}.nii.gz\")\n",
    "\n",
    "    if run_resample_voxel:\n",
    "        all_out_names = []\n",
    "        print(\"applying resampling to bold runs!\")\n",
    "        for d in tqdm(range(len(data))):\n",
    "            resample_func_name = f\"func/{sub}_{session}_task-{func_task_name}_run-{d+1:02}_space-T1w_desc-preproc_bold\"\n",
    "            in_name = os.path.join(datadir, session, f\"{resample_func_name}.nii.gz\")\n",
    "            out_name = f\"{resampled_dir}/{resample_func_name}_{resampled_suffix}.nii.gz\"  # save to the resampled directory and add suffix. don't want to save to fmriprep directory, keep that clean\n",
    "            applyxfm(in_name, ref_name, omat_name, resample_method, output=out_name)\n",
    "            all_out_names.append(out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resample_voxel_size and run_resample_voxel:\n",
    "    # resample the avg_mask\n",
    "    applyxfm(mask_in_name, ref_name, omat_name, resample_method, output=mask_out_name)\n",
    "    apply_thresh(mask_out_name, 0.5, output=mask_out_name)  # binarize the mask since resampling can result in non- 0 or 1 values\n",
    "\n",
    "    # # print information and compare mask to functional data\n",
    "    resampled_mask = nib.load(mask_out_name)\n",
    "    assert set(np.unique(resampled_mask.get_fdata())) <= {0,1}  # make sure mask is boolean\n",
    "    \n",
    "    compare_mask_epi_dims(out_name, resampled_mask)\n",
    "\n",
    "    # now that we've confirmed that the mask and functional data share affines and dimensions, apply new mask to the resampled functional data\n",
    "    print(\"applying resampled mask to resampled functional data\")\n",
    "    for d in tqdm(range(len(data))):\n",
    "        resample_func_name = f\"func/{sub}_{session}_task-{func_task_name}_run-{d+1:02}_space-T1w_desc-preproc_bold\"\n",
    "        out_npy_name = f\"{resampled_dir}/{resample_func_name}_{resampled_suffix}.npy\"\n",
    "        print(out_npy_name)\n",
    "        data[d] = fit_save_bold(all_out_names[d], resampled_mask, out_npy_name, d)\n",
    "    \n",
    "    avg_mask = resampled_mask  # replace the old avg_mask with the resampled mask from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resample_voxel_size and not run_resample_voxel:\n",
    "    for d in range(len(data)):\n",
    "        resample_func_name = f\"func/{sub}_{session}_task-{func_task_name}_run-{d+1:02}_space-T1w_desc-preproc_bold\"\n",
    "        out_npy_name = f\"{resampled_dir}/{resample_func_name}_{resampled_suffix}.npy\"\n",
    "        assert exists(out_npy_name)  # if we want to resample voxel size but not recompute it, the file must exist\n",
    "        print(f\"loading data from: {out_npy_name}\")\n",
    "        data[d] = np.load(out_npy_name)\n",
    "\n",
    "    print(\"\\nresampled data loaded!\")\n",
    "    print(f\"data shape: {data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resample_voxel_size and not run_resample_voxel:\n",
    "    avg_mask = nib.load(mask_out_name)  # load the resampled mask from path to override avg_mask\n",
    "    dimsize, affine_mat, brain, xyz = mask_info(avg_mask, t1_img, plot=True)\n",
    "    print(\"shape:\", brain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsample fMRI data to 1-sec TR\n",
    "At this point, we have extracted all brain voxels and flattened the data to 2D (voxels,TR). \n",
    "Now we will interpolate the time-series to go from 1.5 sec TR to 1 sec TR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(sys.modules['tseriesinterp'])\n",
    "# from tseriesinterp import tseriesinterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel_id=500\n",
    "\n",
    "# f, ax = plt.subplots(1,1, figsize=(14,5))\n",
    "# ax.plot(data[0][voxel_id, :])\n",
    "# ax.set_title('Voxel time series before upsampling, voxel id = %d' % voxel_id)\n",
    "# ax.set_xlabel('TR')\n",
    "# ax.set_ylabel('Voxel Intensity')\n",
    "\n",
    "# f, ax = plt.subplots(1,1, figsize=(14,5))\n",
    "# ax.plot(data_upsampled[voxel_id, :])\n",
    "# ax.set_title('Voxel time series after upsampling, voxel id = %d' % voxel_id)\n",
    "# ax.set_xlabel('TR')\n",
    "# ax.set_ylabel('Voxel Intensity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_upsampled = []\n",
    "TR_run_upsampled = []\n",
    "\n",
    "# if run_upsampling == 1:\n",
    "#     for ses in ses_list:\n",
    "#         for run in range(runs_per_session.get(ses, 0)):\n",
    "#             filename = f\"{outputdir}/{sub}/{sub}_{ses}_task-{func_task_name}_run-{(run+1):02}_2D_upsampled_bold\"\n",
    "#             print(f'\\n*** Saving data to {filename} ***\\n')\n",
    "\n",
    "#             # perform interpolation\n",
    "#             data_new = tseriesinterp(data[run], tr_orig, tr, dim=1)\n",
    "        \n",
    "#             # add this run's data to data variable\n",
    "#             data_upsampled.append(data_new)\n",
    "#             TR_run_upsampled.append(data_new.shape[1])\n",
    "                \n",
    "#             print('BOLD data shape:', data_upsampled[run].shape)\n",
    "#             print('TRs in this run:', TR_run_upsampled[run])\n",
    "        \n",
    "#             # save individual run data to npy file\n",
    "#             np.save(filename, data_new)\n",
    "\n",
    "# elif run_upsampling == 0:\n",
    "#     for ses in ses_list:\n",
    "#         for run in range(runs_per_session.get(ses, 0)):\n",
    "#             filename = f\"{outputdir}/{sub}/{sub}_{ses}_task-{func_task_name}_run-{(run+1):02}_2D_upsampled_bold\"\n",
    "#             print(f'\\n*** Loading data from {filename} ***\\n')\n",
    "#             data_new = np.load(filename)\n",
    "            \n",
    "#             data_upsampled.append(data_new)\n",
    "#             TR_run_upsampled.append(data_new.shape[1])\n",
    "            \n",
    "#             print('BOLD data shape:', data_upsampled[run].shape)\n",
    "#             print('TRs in this run:', TR_run_upsampled[run])\n",
    "\n",
    "for ses in ses_list:\n",
    "    for run in range(runs_per_session.get(ses, 0)):\n",
    "        filename = f\"{outputdir}/{sub}/{sub}_{ses}_task-{func_task_name}_run-{(run+1):02}_2D_upsampled_bold.npy\"\n",
    "        \n",
    "        if run_upsampling == 1:\n",
    "            print(f'\\n*** Saving data to {filename} ***\\n')\n",
    "            # Perform interpolation\n",
    "            data_new = tseriesinterp(data[run], tr_orig, tr, dim=1)\n",
    "            # Save individual run data to npy file\n",
    "            np.save(filename, data_new)\n",
    "        else:\n",
    "            print(f'\\n*** Loading data from {filename} ***\\n')\n",
    "            data_new = np.load(filename)\n",
    "\n",
    "        # Add this run's data to data_upsampled\n",
    "        data_upsampled.append(data_new)\n",
    "        TR_run_upsampled.append(data_new.shape[1])\n",
    "\n",
    "        print('BOLD data shape:', data_new.shape)\n",
    "        print('TRs in this run:', data_new.shape[1])\n",
    "\n",
    "if len(data_upsampled) >= 1:\n",
    "    for d in range(len(data_upsampled)-1):\n",
    "        assert not np.all(data_upsampled[d] == data_upsampled[d+1])\n",
    "print('')\n",
    "print('number of runs in upsampled BOLD data:', len(data_upsampled))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print some relevant metadata\n",
    "print(f'There are {len(data_upsampled)} runs in total\\n')\n",
    "#print(f'N = {data[0].shape[1]} TRs per run\\n')\n",
    "print(f'XYZ dimensionality is: {xyz}\\n')\n",
    "print(f'The dimensions of the 2D data for each run are:')\n",
    "for i in range(len(data_upsampled)):\n",
    "    print(f'{data_upsampled[i].shape}') \n",
    "print(f'The stimulus duration is {stimdur} seconds\\n')\n",
    "print(f'The TR is {tr} seconds\\n')\n",
    "print(f'There are {np.sum(brain)} voxels in the included brain mask\\n')  \n",
    "# print(f'There are {np.sum(roi)} voxels in the included visual ROI\\n')\n",
    "print(f'Numeric precision of data is: {type(data[0][0,0])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot example slice from run 1 and run 16\n",
    "# first, we need to unmask the data to get it back into 3D/4D format:\n",
    "z=round(xyz[2]/3)\n",
    "for runnum in range(n_runs):\n",
    "    X = nilearn.masking.unmask(data_upsampled[runnum].T, avg_mask)\n",
    "    run = X.get_fdata()\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.subplot(121)\n",
    "    plt.title(f'example slice from run {runnum}',fontsize=16)\n",
    "    plt.imshow(run[:,:,z,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot example slice from run 1 and run 16\n",
    "# first, we need to unmask the data to get it back into 3D/4D format:\n",
    "X = nilearn.masking.unmask(data_upsampled[0].T, avg_mask)\n",
    "run1 = X.get_fdata()\n",
    "\n",
    "X = nilearn.masking.unmask(data_upsampled[n_runs-1].T, avg_mask)\n",
    "run16 = X.get_fdata()\n",
    "\n",
    "z=round(xyz[2]/3)\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(run1[:,:,z,0])\n",
    "plt.title('example slice from first run',fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(run16[:,:,z,8])\n",
    "plt.title('example slice from last run',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot design matrix for each run\n",
    "for run in range(n_runs):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(design[run][:,:],interpolation='none')\n",
    "    plt.title(' design matrix from run %i' %(run+1), fontsize=16)\n",
    "    plt.xlabel('conditions',fontsize=16)\n",
    "    plt.ylabel('time (TR)',fontsize=16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GLMsingle with default parameters to estimate single-trial betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# outputs and figures will be stored in a folder (you can specify its name\n",
    "# as the 5th output to GLMsingle). model estimates can be also\n",
    "# saved to the 'results' variable which is the only output of\n",
    "# GLMsingle.\n",
    "\n",
    "# optional parameters below can be assigned to a structure, i.e., opt =\n",
    "# dict('wantlibrary':1, 'wantglmdenoise':1); options are the 6th input to\n",
    "# GLMsingle.\n",
    "\n",
    "# there are many options that can be specified; here, we comment on the\n",
    "# main options that one might want to modify/set. defaults for the options\n",
    "# are indicated below.\n",
    "\n",
    "# wantlibrary = 1 -> fit HRF to each voxel \n",
    "# wantglmdenoise = 1 -> use GLMdenoise \n",
    "# wantfracridge = 1 -> use ridge regression to improve beta estimates \n",
    "# chunklen = 50000 -> is the number of voxels that we will\n",
    "#    process at the same time. for setups with lower memory, you may need to \n",
    "#    decrease this number.\n",
    "\n",
    "# wantmemoryoutputs is a logical vector [A B C D] indicating which of the\n",
    "#     four model types to return in the output <results>. the user must be\n",
    "#     careful with this, as large datasets can require a lot of RAM. if you\n",
    "#     do not request the various model types, they will be cleared from\n",
    "#     memory (but still potentially saved to disk). default: [0 0 0 1]\n",
    "#     which means return only the final type-D model.\n",
    "\n",
    "# wantfileoutputs is a logical vector [A B C D] indicating which of the\n",
    "#     four model types to save to disk (assuming that they are computed). A\n",
    "#     = 0/1 for saving the results of the ONOFF model, B = 0/1 for saving\n",
    "#     the results of the FITHRF model, C = 0/1 for saving the results of the\n",
    "#     FITHRF_GLMdenoise model, D = 0/1 for saving the results of the\n",
    "#     FITHRF_GLMdenoise_RR model. default: [1 1 1 1] which means save all\n",
    "#     computed results to disk.\n",
    "\n",
    "# numpcstotry (optional) is a non-negative integer indicating the maximum\n",
    "#     number of GLMdenoise PCs to enter into the model. default: 10.\n",
    "\n",
    "# fracs (optional) is a vector of fractions that are greater than 0\n",
    "#     and less than or equal to 1. we automatically sort in descending\n",
    "#     order and ensure the fractions are unique. these fractions indicate\n",
    "#     the regularization levels to evaluate using fractional ridge\n",
    "#     regression (fracridge) and cross-validation. default:\n",
    "#     fliplr(.05:.05:1). a special case is when <fracs> is specified as a\n",
    "#     single scalar value. in this case, cross-validation is NOT performed\n",
    "#     for the type-D model, and we instead blindly use the supplied\n",
    "#     fractional value for the type-D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a directory for saving GLMsingle outputs\n",
    "# outputdir_glmsingle = join(homedir,'examples','example1outputs','GLMsingle')\n",
    "outputdir_glmsingle = join(outputdir, sub, f'glmsingle_{session_label}')\n",
    "figuredir_glmsingle = join(outputdir, 'figures', sub)\n",
    "os.makedirs(outputdir_glmsingle,exist_ok=True)\n",
    "os.makedirs(figuredir_glmsingle,exist_ok=True)\n",
    "\n",
    "opt = dict()\n",
    "\n",
    "# set important fields for completeness (but these would be enabled by default)\n",
    "opt['wantlibrary'] = 1\n",
    "opt['wantglmdenoise'] = 1\n",
    "opt['wantfracridge'] = 1\n",
    "\n",
    "# for the purpose of this example we will keep the relevant outputs in memory\n",
    "# and also save them to the disk\n",
    "opt['wantfileoutputs'] = [0,0,0,1]\n",
    "opt['wantmemoryoutputs'] = [1,1,1,1]\n",
    "#opt['wanthdf5'] = 1\n",
    "\n",
    "if session == \"all\":\n",
    "    opt['sessionindicator'] = np.concatenate([\n",
    "        np.full(runs, i + 1)  # 1-based indexing\n",
    "        for i, runs in enumerate(runs_per_session.values())\n",
    "        ])\n",
    "    print(\"***Using sessionindicator because this is a multi-session scan***\", opt['sessionindicator'])\n",
    "\n",
    "# running python GLMsingle involves creating a GLM_single object\n",
    "# and then running the procedure using the .fit() routine\n",
    "glmsingle_obj = GLM_single(opt)\n",
    "\n",
    "# visualize all the hyperparameters\n",
    "pprint(glmsingle_obj.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('shape of data before running GLMsingle:')\n",
    "print(data_upsampled[0].shape)\n",
    "print('shape of design matrix:')\n",
    "print(design[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try running with brainexclude=avg_mask\n",
    "# if it crashes, then increase max to 1,500,000\n",
    "\n",
    "# this example saves output files to the outputdir folder\n",
    "# if these outputs don't already exist, we will perform the time-consuming call to GLMsingle;\n",
    "# otherwise, we will just load from disk.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#if not exists(outputdir_glmsingle):\n",
    "if run_glmsingle == 1:\n",
    "    print('saving outputs to', outputdir_glmsingle)\n",
    "    print(f'running GLMsingle...')\n",
    "    \n",
    "    # run GLMsingle\n",
    "    results_glmsingle = glmsingle_obj.fit(\n",
    "       design,\n",
    "       data_upsampled,\n",
    "       stimdur,\n",
    "       tr,\n",
    "       outputdir=outputdir_glmsingle,\n",
    "       figuredir=figuredir_glmsingle\n",
    "       )\n",
    "    \n",
    "    # save results in memory to npz\n",
    "    np.savez(join(outputdir_glmsingle,'TYPEA_ONOFF.npz'), **results_glmsingle['typea'])\n",
    "    np.savez(join(outputdir_glmsingle,'TYPEB_FITHRF.npz'), **results_glmsingle['typeb'])\n",
    "    np.savez(join(outputdir_glmsingle,'TYPEC_FITHRF_GLMDENOISE.npz'), **results_glmsingle['typec'])\n",
    "    np.savez(join(outputdir_glmsingle,'TYPED_FITHRF_GLMDENOISE_RR.npz'), **results_glmsingle['typed'])\n",
    "    \n",
    "    # we assign outputs of GLMsingle to the \"results_glmsingle\" variable.\n",
    "    # note that results_glmsingle['typea'] contains GLM estimates from an ONOFF model,\n",
    "    # where all images are treated as the same condition. these estimates\n",
    "    # could be potentially used to find cortical areas that respond to\n",
    "    # visual stimuli. we want to compare beta weights between conditions\n",
    "    # therefore we are not going to include the ONOFF betas in any analyses of \n",
    "    # voxel reliability\n",
    "    \n",
    "elif run_glmsingle == 0:\n",
    "    print(f'loading existing GLMsingle outputs from directory:\\n\\t{outputdir_glmsingle}')\n",
    "    # takes about 2 minutes to load\n",
    "    \n",
    "    # load existing file outputs if they exist\n",
    "    results_glmsingle = dict()\n",
    "    \n",
    "    if opt['wanthdf5'] == 1: #if outputs are in hdf5 format\n",
    "        results_intm = dict()\n",
    "        results_intm['typea'] = h5py.File(join(outputdir_glmsingle,'TYPEA_ONOFF.hdf5'), 'r')\n",
    "        results_intm['typeb'] = h5py.File(join(outputdir_glmsingle,'TYPEB_FITHRF.hdf5'), 'r')\n",
    "        results_intm['typec'] = h5py.File(join(outputdir_glmsingle,'TYPEC_FITHRF_GLMDENOISE.hdf5'), 'r')\n",
    "        results_intm['typed'] = h5py.File(join(outputdir_glmsingle,'TYPED_FITHRF_GLMDENOISE_RR.hdf5'), 'r')\n",
    "        \n",
    "        model_types = ['typea', 'typeb', 'typec', 'typed']\n",
    "        for model in range(len(model_types)):\n",
    "            results_glmsingle[model_types[model]] = dict()\n",
    "            plot_fields = list(results_intm[model_types[model]].keys())\n",
    "    \n",
    "            for i in range(len(plot_fields)):\n",
    "                results_glmsingle[model_types[model]][plot_fields[i]] = results_intm[model_types[model]].get(plot_fields[i]).value\n",
    "    \n",
    "    else: #load npy files\n",
    "        #results_glmsingle['typea'] = np.load(join(outputdir_glmsingle,'TYPEA_ONOFF.npy'),allow_pickle=True).item()\n",
    "        #results_glmsingle['typeb'] = np.load(join(outputdir_glmsingle,'TYPEB_FITHRF.npy'),allow_pickle=True).item()\n",
    "        #results_glmsingle['typec'] = np.load(join(outputdir_glmsingle,'TYPEC_FITHRF_GLMDENOISE.npy'),allow_pickle=True).item()\n",
    "        #results_glmsingle['typed'] = np.load(join(outputdir_glmsingle,'TYPED_FITHRF_GLMDENOISE_RR.npy'),allow_pickle=True).item()\n",
    "\n",
    "        # outputdir_glmsingle = '/jukebox/norman/rsiyer/rtmindeye/data/bids/derivatives/glmsingle/sub-003/glmsingle_ses-01'\n",
    "        results_glmsingle['typea'] = np.load(join(outputdir_glmsingle,'TYPEA_ONOFF.npz'))\n",
    "        results_glmsingle['typeb'] = np.load(join(outputdir_glmsingle,'TYPEB_FITHRF.npz'))\n",
    "        results_glmsingle['typec'] = np.load(join(outputdir_glmsingle,'TYPEC_FITHRF_GLMDENOISE.npz'))\n",
    "        results_glmsingle['typed'] = np.load(join(outputdir_glmsingle,'TYPED_FITHRF_GLMDENOISE_RR.npz'))\n",
    "        \n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\n",
    "    '\\telapsed time: ',\n",
    "    f'{time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_glmsingle['typea']['onoffR2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_mask.get_fdata().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of important outputs\n",
    "\n",
    "the outputs of GLMsingle are formally documented in its header. here, we highlight a few of the more important outputs:\n",
    "\n",
    "* R2 -> is model accuracy expressed in terms of R^2 (percentage).\n",
    "\n",
    "* betasmd -> is the full set of single-trial beta weights (X x Y x Z x TRIALS). beta weights are arranged in chronological order.\n",
    "\n",
    "* HRFindex -> is the 1-index of the best fit HRF. HRFs can be recovered with getcanonicalHRFlibrary(stimdur,tr)\n",
    "\n",
    "* FRACvalue -> is the fractional ridge regression regularization level chosen for each voxel. values closer to 1 mean less regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GLMsingle output is in 2D format (voxels,samples)\n",
    "# we need to unmask the data to get it back into 3D/4D format:\n",
    "# X = nilearn.masking.unmask(results_glmsingle['typeb']['HRFindex'], avg_mask)\n",
    "# X_hrf = X.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_out = {\n",
    "#                     'onoffR2': np.reshape(onoffR2, [nx, ny, nz]),\n",
    "#                     'meanvol': np.reshape(meanvol, [nx, ny, nz]),\n",
    "#                     'betasmd': np.reshape(betasmd, [nx, ny, nz])\n",
    "#                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert everything back to 3D/4D array format\n",
    "model_types = ['typea','typeb','typec','typed']\n",
    "\n",
    "results_glmsingle_3D = dict()\n",
    "\n",
    "for model in range(len(model_types)):\n",
    "    results_glmsingle_3D[model_types[model]] = dict()\n",
    "    \n",
    "    #plot_fields = list(results_glmsingle[model_types[model]].keys())\n",
    "    #print(model, plot_fields)\n",
    "    \n",
    "    if model == 0: #type A\n",
    "        plot_fields = ['betasmd','meanvol','onoffR2']\n",
    "    elif model == 1: #type B\n",
    "        plot_fields = ['betasmd','meanvol', 'R2','HRFindex', 'FitHRFR2']\n",
    "    elif model == 2: #type C\n",
    "        plot_fields = ['betasmd','meanvol', 'R2','HRFindex']\n",
    "    elif model == 3: #type D\n",
    "        plot_fields = ['betasmd','meanvol', 'R2','HRFindex','FRACvalue']\n",
    "    print(model_types[model])\n",
    "    \n",
    "    for i in range(len(plot_fields)):\n",
    "        print(plot_fields[i])\n",
    "        # find time dimension\n",
    "        dim = len(results_glmsingle[model_types[model]][plot_fields[i]].shape)-1\n",
    "        # flatten to 2D\n",
    "        mflat = reshape2D(results_glmsingle[model_types[model]][plot_fields[i]],dim) \n",
    "        print('original shape:', results_glmsingle[model_types[model]][plot_fields[i]].shape)\n",
    "        print('2D shape:', mflat.shape)\n",
    "        # if sub == 'sub-001' and session == 'ses-01':\n",
    "        #     unmask_data = nilearn.masking.unmask(results_glmsingle[model_types[model]][plot_fields[i]].T, avg_mask)\n",
    "        # else:\n",
    "        unmask_data = nilearn.masking.unmask(mflat, avg_mask)\n",
    "        results_glmsingle_3D[model_types[model]][plot_fields[i]] = unmask_data.get_fdata()\n",
    "        print('shape of unmasked data:', results_glmsingle_3D[model_types[model]][plot_fields[i]].shape)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a slice of brain showing GLMsingle outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_glmsingle_3D['typeb'][plot_fields[i]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we are going to plot several outputs from the FIT_HRF_GLMdenoise_RR GLM,\n",
    "# # which contains the full set of GLMsingle optimizations.\n",
    "\n",
    "# # choose a slice\n",
    "# #z=round(xyz[2]/3)\n",
    "# z=35\n",
    "\n",
    "# # we will plot betas, R2, optimal HRF indices, and the voxel frac values\n",
    "# plot_fields = ['betasmd','R2','HRFindex','FRACvalue']\n",
    "# #plot_fields = ['betasmd']\n",
    "# colormaps = ['RdBu_r','hot','jet','copper']\n",
    "# clims = [[-5,5],[0,85],[0,20],[0,1]]\n",
    "\n",
    "# plt.figure(figsize=(12,8))\n",
    "\n",
    "# for i in range(len(plot_fields)):\n",
    "    \n",
    "#     plt.subplot(2,2,i+1)\n",
    "    \n",
    "#     if i == 0:\n",
    "#         # when plotting betas, for simplicity just average across all image presentations\n",
    "#         # this will yield a summary of whether voxels tend to increase or decrease their \n",
    "#         # activity in response to the experimental stimuli (similar to outputs from \n",
    "#         # an ONOFF GLM)\n",
    "#         plot_data = np.nanmean(np.squeeze(results_assumehrf_3D['typeb'][plot_fields[i]]),3)\n",
    "#         titlestr = 'average GLM betas (1000 trials)'\n",
    "    \n",
    "#     else:\n",
    "#         # plot all other voxel-wise metrics as outputted from GLMsingle\n",
    "#         plot_data = np.squeeze(results_assumehrf_3D['typeb'][plot_fields[i]].reshape(xyz))\n",
    "#         titlestr = plot_fields[i]\n",
    "    \n",
    "#     plt.imshow(plot_data[:,:,z],cmap=colormaps[i],clim=clims[i])\n",
    "#     plt.colorbar()\n",
    "#     plt.title(titlestr)\n",
    "#     plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we are going to plot several outputs from the FIT_HRF_GLMdenoise_RR GLM,\n",
    "# # which contains the full set of GLMsingle optimizations.\n",
    "\n",
    "# # choose a slice\n",
    "# #z=round(xyz[2]/3)\n",
    "# z=30\n",
    "\n",
    "# # we will plot betas, R2, optimal HRF indices, and the voxel frac values\n",
    "# plot_fields = ['betasmd','R2','HRFindex','FRACvalue']\n",
    "# #plot_fields = ['betasmd']\n",
    "# colormaps = ['RdBu_r','hot','jet','copper']\n",
    "# clims = [[-5,5],[0,85],[0,20],[0,1]]\n",
    "\n",
    "# plt.figure(figsize=(12,8))\n",
    "\n",
    "\n",
    "\n",
    "# if i == 0:\n",
    "#     # when plotting betas, for simplicity just average across all image presentations\n",
    "#     # this will yield a summary of whether voxels tend to increase or decrease their \n",
    "#     # activity in response to the experimental stimuli (similar to outputs from \n",
    "#     # an ONOFF GLM)\n",
    "#     plot_data = np.nanmean(np.squeeze(results_glmsingle_3D['typed'][plot_fields[i]]),3)\n",
    "#     titlestr = 'average GLM betas (1000 trials)'\n",
    "    \n",
    "# plt.imshow(plot_data[:,:,z],cmap=colormaps[i],clim=clims[i])\n",
    "# plt.colorbar()\n",
    "# plt.title(titlestr)\n",
    "# plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.all(np.squeeze(results_glmsingle_3D['typec']['betasmd']) == results_glmsingle_3D['typec']['betasmd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_assumehrf_3D['typeb']['betasmd'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data = np.nanmean(np.squeeze(results_glmsingle_3D['typeb']['betasmd']),3)\n",
    "# plt.imshow(plot_data[:, :, 40],clim=[-.4,.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.nanmean(np.squeeze(results_glmsingle_3D['typea']['betasmd']),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we are going to plot several outputs from the FIT_HRF_GLMdenoise_RR GLM,\n",
    "# # which contains the full set of GLMsingle optimizations.\n",
    "\n",
    "# # choose a slice\n",
    "# #z=round(xyz[2]/3)\n",
    "# z=48\n",
    "\n",
    "# # we will plot betas, R2, optimal HRF indices, and the voxel frac values\n",
    "# plot_fields = ['betasmd','R2','HRFindex','FRACvalue']\n",
    "# #plot_fields = ['betasmd']\n",
    "# colormaps = ['RdBu_r','hot','jet','copper']\n",
    "# clims = [[-0.5,0.5],[0,20],[0,20],[0,1]]\n",
    "\n",
    "# plt.figure(figsize=(12,8))\n",
    "\n",
    "# for i in range(len(plot_fields)):\n",
    "    \n",
    "#     plt.subplot(2,2,i+1)\n",
    "    \n",
    "#     if i == 0:\n",
    "#         # when plotting betas, for simplicity just average across all image presentations\n",
    "#         # this will yield a summary of whether voxels tend to increase or decrease their \n",
    "#         # activity in response to the experimental stimuli (similar to outputs from \n",
    "#         # an ONOFF GLM)\n",
    "#         plot_data = np.nanmean(np.squeeze(results_glmsingle_3D['typed'][plot_fields[i]]),3)\n",
    "#         titlestr = 'average GLM betas (1000 trials)'\n",
    "    \n",
    "#     else:\n",
    "#         # plot all other voxel-wise metrics as outputted from GLMsingle\n",
    "#         plot_data = np.squeeze(results_glmsingle_3D['typed'][plot_fields[i]].reshape(xyz))\n",
    "#         titlestr = plot_fields[i]\n",
    "    \n",
    "#     plt.imshow(plot_data[:,:,z],cmap=colormaps[i],clim=clims[i])\n",
    "#     plt.colorbar()\n",
    "#     plt.title(titlestr)\n",
    "#     plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to plot several outputs from the FIT_HRF_GLMdenoise_RR GLM,\n",
    "# which contains the full set of GLMsingle optimizations.\n",
    "dimsize, affine_mat, brain, xyz = mask_info(avg_mask, t1_img, plot=True)\n",
    "\n",
    "# choose a slice\n",
    "z=round(xyz[2]/3)\n",
    "# z=38\n",
    "\n",
    "# we will plot betas, R2, optimal HRF indices, and the voxel frac values\n",
    "plot_fields = ['betasmd','R2','HRFindex','FRACvalue']\n",
    "#plot_fields = ['betasmd']\n",
    "colormaps = ['RdBu_r','hot','jet','copper']\n",
    "clims = [[-5,5],[0,85],[0,20],[0,1]]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i in range(len(plot_fields)):\n",
    "    \n",
    "    plt.subplot(2,2,i+1)\n",
    "    \n",
    "    if i == 0:\n",
    "        # when plotting betas, for simplicity just average across all image presentations\n",
    "        # this will yield a summary of whether voxels tend to increase or decrease their \n",
    "        # activity in response to the experimental stimuli (similar to outputs from \n",
    "        # an ONOFF GLM)\n",
    "        plot_data = np.nanmean(np.squeeze(results_glmsingle_3D['typed'][plot_fields[i]]),3)\n",
    "        titlestr = 'average GLM betas (1000 trials)'\n",
    "    \n",
    "    else:\n",
    "        # plot all other voxel-wise metrics as outputted from GLMsingle\n",
    "        plot_data = np.squeeze(results_glmsingle_3D['typed'][plot_fields[i]].reshape(xyz))\n",
    "        titlestr = plot_fields[i]\n",
    "    \n",
    "    plt.imshow(plot_data[:,:,z],cmap=colormaps[i],clim=clims[i])\n",
    "    plt.colorbar()\n",
    "    plt.title(titlestr)\n",
    "    plt.axis(False)\n",
    "    nib.save(nib.Nifti1Image(plot_data, affine_mat), f\"{outputdir_glmsingle}/beta_map_{plot_fields[i]}\")\n",
    "\n",
    "plt.savefig(f'{figuredir_glmsingle}/glmsingle_outputs_slice{z}')\n",
    "plt.show()\n",
    "\n",
    "print(\"saved betas as nifti image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run a baseline GLM to compare with GLMsingle outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for comparison purposes we are going to run a standard GLM\n",
    "# without HRF fitting, GLMdenoise, or ridge regression regularization. we\n",
    "# will compute the split-half reliability at each voxel using this baseline\n",
    "# GLM, and then assess whether reliability improves using the output betas\n",
    "# from GLMsingle. \n",
    "\n",
    "# output directory for baseline GLM\n",
    "outputdir_baseline = join(outputdir, sub, 'glmbaseline')\n",
    "os.makedirs(outputdir_baseline,exist_ok=True)\n",
    "\n",
    "# we will run this baseline GLM by changing the default settings in GLMsingle \n",
    "# contained within the \"opt\" structure.\n",
    "opt = dict() \n",
    "\n",
    "# turn off optimizations \n",
    "opt['wantlibrary'] = 0 # switch off HRF fitting\n",
    "opt['wantglmdenoise'] = 0 # switch off GLMdenoise\n",
    "opt['wantfracridge'] = 0 # switch off ridge regression\n",
    "\n",
    "# for the purpose of this example we will keep the relevant outputs in memory\n",
    "# and also save them to the disk...\n",
    "# the first two indices are the ON-OFF GLM and the baseline single-trial GLM. \n",
    "# no need to save the third (+ GLMdenoise) and fourth (+ fracridge) outputs\n",
    "# since they will not even be computed\n",
    "\n",
    "opt['wantmemoryoutputs'] = [1,1,0,0] \n",
    "opt['wantfileoutputs'] = [1,1,0,0]\n",
    "#opt['wanthdf5'] = 1\n",
    "\n",
    "# running python GLMsingle involves creating a GLM_single object\n",
    "# and then running the procedure using the .fit() routine\n",
    "glmbaseline_obj = GLM_single(opt)\n",
    "\n",
    "# visualize the hyperparameters, including the modified baseline opts\n",
    "pprint(glmbaseline_obj.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# if these outputs don't already exist, we will perform the call to\n",
    "# GLMsingle; otherwise, we will just load from disk.\n",
    "#if not exists(outputdir_baseline):\n",
    "if run_glmbaseline == 1:\n",
    "    \n",
    "    print(f'running GLMsingle...')\n",
    "\n",
    "    # run GLMsingle, fitting the baseline GLM\n",
    "    results_assumehrf = glmbaseline_obj.fit(\n",
    "       design,\n",
    "       data_upsampled,\n",
    "       stimdur,\n",
    "       tr,\n",
    "       outputdir=outputdir_baseline)\n",
    "    \n",
    "    # save results in memory to npz\n",
    "    np.savez(join(outputdir_baseline,'TYPEA_ONOFF.npz'), **results_assumehrf['typea'])\n",
    "    np.savez(join(outputdir_baseline,'TYPEB_FITHRF.npz'), **results_assumehrf['typeb'])\n",
    "\n",
    "elif run_glmbaseline == 0:\n",
    "    \n",
    "    print(f'loading existing GLMsingle outputs from directory:\\n\\t{outputdir_baseline}')\n",
    "    \n",
    "    results_assumehrf = dict()\n",
    "    \n",
    "    if opt['wanthdf5'] == 1: #if outputs are in hdf5 format\n",
    "        results_intm2 = dict()\n",
    "        results_intm2['typea'] = h5py.File(join(outputdir_baseline,'TYPEA_ONOFF.hdf5'), 'r')\n",
    "        results_intm2['typeb'] = h5py.File(join(outputdir_baseline,'TYPEB_FITHRF.hdf5'), 'r')\n",
    "        \n",
    "        model_types = ['typea', 'typeb']\n",
    "        for model in range(len(model_types)):\n",
    "            results_assumehrf[model_types[model]] = dict()\n",
    "            plot_fields = list(results_intm2[model_types[model]].keys())\n",
    "    \n",
    "            for i in range(len(plot_fields)):\n",
    "                results_assumehrf[model_types[model]][plot_fields[i]] = results_intm2[model_types[model]].get(plot_fields[i]).value\n",
    "    else:\n",
    "        #results_assumehrf['typea'] = np.load(join(outputdir_baseline,'TYPEA_ONOFF.npy'),allow_pickle=True).item()\n",
    "        #results_assumehrf['typeb'] = np.load(join(outputdir_baseline,'TYPEB_FITHRF.npy'),allow_pickle=True).item()\n",
    "        results_assumehrf['typea'] = np.load(join(outputdir_baseline,'TYPEA_ONOFF.npz'))\n",
    "        results_assumehrf['typeb'] = np.load(join(outputdir_baseline,'TYPEB_FITHRF.npz'))\n",
    "    \n",
    "    # note that even though we are loading TYPEB_FITHRF betas, HRF fitting\n",
    "    # has been turned off and this struct field will thus contain the\n",
    "    # outputs of a GLM fit using the canonical HRF.\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\n",
    "    '\\telapsed time: ',\n",
    "    f'{time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert back to 3D/4D array format\n",
    "model_types = ['typea','typeb']\n",
    "\n",
    "results_assumehrf_3D = dict()\n",
    "\n",
    "for model in range(len(model_types)):\n",
    "    results_assumehrf_3D[model_types[model]] = dict()\n",
    "    if model == 0: #type A\n",
    "        plot_fields = ['betasmd','onoffR2']\n",
    "    elif model == 1: #type B\n",
    "        plot_fields = ['betasmd','R2','HRFindex']\n",
    "    print(model_types[model])\n",
    "    for i in range(len(plot_fields)):\n",
    "        print(plot_fields[i])\n",
    "        # find time dimension\n",
    "        dim = len(results_assumehrf[model_types[model]][plot_fields[i]].shape)-1\n",
    "        # flatten to 2D\n",
    "        mflat = reshape2D(results_assumehrf[model_types[model]][plot_fields[i]],dim) \n",
    "        print('original shape:', results_assumehrf[model_types[model]][plot_fields[i]].shape)\n",
    "        print('2D shape:', mflat.shape)\n",
    "        # if sub == 'sub-001' and session == 'ses-01':\n",
    "        #     unmask_data = nilearn.masking.unmask(results_assumehrf[model_types[model]][plot_fields[i]].T, avg_mask)\n",
    "        # else:\n",
    "        unmask_data = nilearn.masking.unmask(mflat, avg_mask)\n",
    "        results_assumehrf_3D[model_types[model]][plot_fields[i]] = unmask_data.get_fdata()\n",
    "        print('shape of unmasked data:', results_assumehrf_3D[model_types[model]][plot_fields[i]].shape)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dictionary containing the GLM betas from the four different models we will compare.\n",
    "# note that the \"assume hrf\" betas come from the \"typeb\" field of our baseline GLM\n",
    "# (with HRF fitting turned off), and that the \"fit hrf\" betas also come from \n",
    "# the \"typeb\" field of the GLM that ran with all default GLMsingle routines\n",
    "# enabled\n",
    "\n",
    "models = dict()\n",
    "models['assumehrf'] = results_assumehrf_3D['typeb']['betasmd']#.reshape(xyz + (1000,))\n",
    "models['fithrf'] = results_glmsingle_3D['typeb']['betasmd']\n",
    "models['fithrf_glmdenoise'] = results_glmsingle_3D['typec']['betasmd']\n",
    "models['fithrf_glmdenoise_rr'] = results_glmsingle_3D['typed']['betasmd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get indices of repeated conditions to use for reliability calculations\n",
    "\n",
    "To compare the results of different GLMs we are going to calculate the voxel-wise split-half reliablity for each model. Reliability values reflect a correlation between beta weights for repeated presentations of the same conditions. In short, we are going to check how reliable/reproducible are the single trial responses to repeated conditions estimated with each GLM type.\n",
    "\n",
    "In the code below, we are attempting to locate the indices in the beta weight GLMsingle outputs modelmd(x,y,z,trials) that correspond to repeated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_reliability == 1:\n",
    "    \n",
    "    # consolidate design matrices\n",
    "    designALL = np.concatenate(design,axis=0)\n",
    "    print('designALL shape (TRs, conditions):', designALL.shape)\n",
    "\n",
    "    # construct a vector containing 0-indexed condition numbers in chronological order\n",
    "    corder = []\n",
    "    for p in range(designALL.shape[0]):\n",
    "        if np.any(designALL[p]):\n",
    "            corder.append(np.argwhere(designALL[p])[0,0])\n",
    "\n",
    "    corder = np.array(corder)\n",
    "    \n",
    "    # let's take a look at the first few entries\n",
    "    print(corder[:3])\n",
    "    \n",
    "    # note that [374 496 7] means that the first stimulus trial involved\n",
    "    # presentation of the 374th condition (zero-indexed), the second stimulus trial \n",
    "    # involved presentation of the 496th condition, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in order to compute split-half reliability, we have to do some indexing.\n",
    "# we want to find images with least two repetitions and then prepare a\n",
    "# useful matrix of indices that refer to when these occur.\n",
    "\n",
    "if run_reliability == 1:\n",
    "    \n",
    "    repindices = [] # 2 x images containing stimulus trial indices.\n",
    "\n",
    "    # the first row refers to the first presentation; the second row refers to\n",
    "    # the second presentation.\n",
    "    for p in range(designALL.shape[1]): # loop over every condition\n",
    "\n",
    "        temp = np.argwhere(corder==p)[:,0] # find indices where this condition was shown\n",
    "        #print(len(temp)) #vio condition should have 11 As, nonvio should have 9 As\n",
    "        \n",
    "        # only add repeated conditions to repindices\n",
    "        if len(temp) >= 2:\n",
    "            repindices.append([temp[0], temp[1]]) # note that for now we only look at first 2 repetitions\n",
    "\n",
    "    repindices = np.vstack(np.array(repindices)).T\n",
    "    print(repindices.shape, '(repetitions, condition)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_reliability == 1:\n",
    "    # let's take a look at a few entries\n",
    "    print(repindices[:,:3])\n",
    "    \n",
    "    # notice that the first condition is presented on the 216th zero-indexed \n",
    "    # stimulus trial and the 485th stimulus trial, the second condition is presented on the\n",
    "    # 217th and 620st stimulus trials, and so on.\n",
    "    \n",
    "    print(f'there are {repindices.shape[1]} repeated conditions in the experiment')\n",
    "\n",
    "# now, for each voxel we are going to correlate beta weights describing the\n",
    "# response to images presented for the first time with beta weights\n",
    "# describing the response from the repetition of the same image. with 136\n",
    "# repeated conditions, the correlation for each voxel will reflect the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute median split-half reliabilty within the ROI for each beta version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finally, let's compute split-half reliability. we are going to loop\n",
    "# through our 4 models and calculate split-half reliability for each of them\n",
    "if run_reliability == 1:\n",
    "    \n",
    "    vox_reliabilities = [] # output variable for reliability values\n",
    "\n",
    "    modelnames = list(models.keys())#[1:]\n",
    "    print(modelnames)\n",
    "\n",
    "    # for each beta version...\n",
    "    for m in range(len(modelnames)):\n",
    "\n",
    "        print(f'computing reliability for beta version: {modelnames[m]}')\n",
    "        time.sleep(1)\n",
    "\n",
    "        # get the repeated-condition GLM betas using our repindices variable\n",
    "        betas = models[modelnames[m]][:,:,:,repindices] # automatically reshapes to (X x Y x Z x 2 x nConditions)\n",
    "        # betas = models[modelnames[m]]['betasmd'][:,None,None,:][:,:,:,repindices] # automatically reshapes to (X x Y x Z x 2 x nConditions)\n",
    "        x,y,z = betas.shape[:3] \n",
    "\n",
    "        rels = np.full((x,y,z),np.nan)\n",
    "\n",
    "        # loop through voxels in the 3D volume...\n",
    "        for xx in tqdm(range(x)):\n",
    "            for yy in range(y):\n",
    "                for zz in range(z):\n",
    "\n",
    "                    # reliability at a given voxel is pearson correlation between response profiles from first and \n",
    "                    # second image presentations (dim = 136 conditions)\n",
    "                    rels[xx,yy,zz] = np.corrcoef(betas[xx,yy,zz,0],\n",
    "                                                 betas[xx,yy,zz,1])[1,0]\n",
    "\n",
    "        vox_reliabilities.append(rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mask defining liberal visual cortex ROI\n",
    "mask = 'nsdgeneral'\n",
    "mask_file=maskdir + '/%s_%s_%s%s.nii.gz' % (sub,session,task,mask)\n",
    "# print(mask_file)\n",
    "\n",
    "nsd_masks = []\n",
    "for ses in ses_list:\n",
    "    mask_name = f\"{maskdir}/{sub}_{ses}_task-{func_task_name}_{mask}.nii.gz\"\n",
    "    print(mask_name)\n",
    "    nsd_masks.append(mask_name)\n",
    "\n",
    "nsd_masks = [nilearn.image.load_img(mask) for mask in nsd_masks]\n",
    "mask_data = intersect_masks(nsd_masks, threshold=0.5, connected=True)\n",
    "# Apply brain mask to ensure voxels in mask_data are within brain\n",
    "mask_data = nilearn.image.math_img('img1 * img2', img1=mask_data, img2=avg_mask)\n",
    "\n",
    "if resample_voxel_size:\n",
    "    mask_file_resampled = resampled_dir + '/%s_%s_%s%s_resampled.nii.gz' % (sub,session,task,mask)\n",
    "    if run_resample_voxel:\n",
    "        applyxfm(mask_file, ref_name, omat_name, resample_method, output=mask_file_resampled)\n",
    "    mask_file = mask_file_resampled  # ensure you load the resampled file instead\n",
    "    print(mask_file)\n",
    "    \n",
    "# mask_data=nib.load(mask_file)\n",
    "roi=mask_data.get_fdata()\n",
    "print('roi shape:', roi.shape)\n",
    "print(np.min(roi), np.max(roi))\n",
    "\n",
    "roi = roi.astype(float)\n",
    "# convert voxels outside ROI to nan for overlay plotting\n",
    "roi[roi==0] = np.nan \n",
    "print(np.nanmin(roi), np.nanmax(roi))\n",
    "print(roi.shape)\n",
    "\n",
    "roi = roi.flatten()\n",
    "brain_mask = avg_mask.get_fdata().flatten()\n",
    "roi = roi[brain_mask.astype(bool)]\n",
    "roi[np.isnan(roi)] = 0\n",
    "roi = roi.astype(bool)\n",
    "print(roi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save voxel reliabilities in NSDgeneral ROI\n",
    "if run_reliability == 1:\n",
    "    vox_rel_nsdgeneral = []\n",
    "    for vr in vox_reliabilities:\n",
    "        vox_rel_nsdgeneral.append(vr[avg_mask.get_fdata()==1][roi==True])\n",
    "        \n",
    "    np.save(join(savedir,'voxel_reliabilities_NSDgeneral.npy'), vox_rel_nsdgeneral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess change in reliability yielded by GLMsingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each GLM we will calculate median reliability for voxels within the\n",
    "# nsdgeneral visual ROI and compare using a bar graph\n",
    "if run_reliability == 1:\n",
    "    comparison = []\n",
    "    for vr in vox_reliabilities:\n",
    "        # comparison.append(np.nanmedian(vr)) \n",
    "        comparison.append(np.nanmedian(vr[avg_mask.get_fdata()==1][roi==True]))\n",
    "\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.subplot(121)\n",
    "    plt.bar(np.arange(len(comparison)),comparison,width=0.5)\n",
    "    plt.title('Median voxel split-half reliability of GLM models')\n",
    "    plt.xticks(np.arange(4),np.array(['ASSUMEHRF','FITHRF', 'FITHRF\\nGLMDENOISE', 'FITHRF\\nGLMDENOISE\\nRR']))\n",
    "    #plt.ylim([0.1,0.2])\n",
    "    plt.savefig(f'{figuredir_glmsingle}/split-half_reliability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['v1'] #glmsingle versions\n",
    "\n",
    "info = dict()\n",
    "info['v1'] = dict()\n",
    "info['v1']['subjs'] = ['sub-001']\n",
    "info['methods'] = ['assumehrf', 'fithrf', 'fithrf_glmdenoise', 'fithrf_glmdenoise_rr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability = dict()\n",
    "\n",
    "#masks = np.load('masks.npy',allow_pickle=True).item() \n",
    "\n",
    "# check to be sure all the datafiles exist\n",
    "for ds in datasets:\n",
    "    \n",
    "    reliability[ds] = dict()\n",
    "    \n",
    "    for subj in info[ds]['subjs']:\n",
    "        \n",
    "        subj_version_list = []\n",
    "        \n",
    "        reliability[ds][subj] = dict()\n",
    "\n",
    "        # load this versions's data\n",
    "        if ds == 'v1':    \n",
    "            temp = np.load(join(savedir,'voxel_reliabilities_NSDgeneral.npy')) \n",
    "            method_list = ['assumehrf', 'fithrf', 'fithrf_glmdenoise', 'fithrf_glmdenoise_rr']\n",
    "        \n",
    "        for method in method_list:\n",
    "            if method == 'assumehrf':\n",
    "                idx=0\n",
    "            elif method == 'fithrf':\n",
    "                idx=1\n",
    "            elif method == 'fithrf_glmdenoise':\n",
    "                idx=2\n",
    "            elif method == 'fithrf_glmdenoise_rr':\n",
    "                if ds == 'v3':\n",
    "                    idx=2\n",
    "                else:\n",
    "                    idx=3\n",
    "\n",
    "            rel = temp[idx]\n",
    "            #rel = np.load(metric_savefn)\n",
    "                    \n",
    "            #mask = masks[ds][subj]==1\n",
    "            \n",
    "            #if np.ndim(rel) == 3:\n",
    "                #rel = rel[mask==1]\n",
    "                \n",
    "            print(ds,method,subj,rel.shape)\n",
    "            \n",
    "            reliability[ds][subj][method] = rel\n",
    "            \n",
    "            subj_version_list.append(rel)\n",
    "                       \n",
    "        reliability[ds][subj]['mean'] = np.mean(np.stack(subj_version_list,axis=1),axis=1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plots = True\n",
    "\n",
    "if draw_plots: \n",
    "    \n",
    "    x_version = 'assumehrf'\n",
    "    y_version = 'fithrf_glmdenoise_rr'\n",
    "\n",
    "    # iterate through datasets\n",
    "    for ds in datasets:\n",
    "        for subj in info[ds]['subjs']:\n",
    "\n",
    "            #x = helpers.reshape_nsdgeneral_to_volume(reliability[ds][subj][x_version], masks[ds][subj])\n",
    "            #y = helpers.reshape_nsdgeneral_to_volume(reliability[ds][subj][y_version], masks[ds][subj])\n",
    "            x = reliability[ds][subj][x_version]\n",
    "            y = reliability[ds][subj][y_version]\n",
    "\n",
    "            #x[masks[ds][subj] < 1] = np.nan\n",
    "            #y[masks[ds][subj] < 1] = np.nan\n",
    "\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.grid('on')\n",
    "            plt.scatter(x, y, 10, alpha = 0.5)\n",
    "            plt.axis('square')\n",
    "            plt.xlabel(f\"{x_version} reliability\")\n",
    "            plt.ylabel(f\"{y_version} reliability\")\n",
    "\n",
    "            plt.plot(np.arange(-0.6,0.7,0.01),np.arange(-0.6,0.7,0.01),'r',linewidth=2.5,zorder=10)\n",
    "\n",
    "            if ds == 'mindeye':\n",
    "                ticks = (-0.3,0.8,0.4)\n",
    "            else:\n",
    "                ticks = (-0.3,1,0.4)\n",
    "\n",
    "            plt.xticks(np.round(np.arange(ticks[0],ticks[1],ticks[2]),2),\n",
    "                       np.round(np.arange(ticks[0],ticks[1],ticks[2]),2),fontsize=20)\n",
    "            plt.yticks(np.round(np.arange(ticks[0],ticks[1],ticks[2]),2),\n",
    "                       np.round(np.arange(ticks[0],ticks[1],ticks[2]),2),fontsize=20)\n",
    "\n",
    "            if ds == 'mindeye':\n",
    "                plt.xlim([-0.5,0.75])\n",
    "                plt.ylim([-0.5,0.75])\n",
    "            else:\n",
    "                plt.xlim([-0.5,1])\n",
    "                plt.ylim([-0.5,1])\n",
    "\n",
    "            # get rid of the frame\n",
    "            for spine in plt.gca().spines.values():\n",
    "                spine.set_visible(False)\n",
    "\n",
    "            plt.savefig(f\"{figuredir_glmsingle}/scatter.jpg\", bbox_inches='tight', dpi=150)  # https://stackoverflow.com/questions/19576317/matplotlib-savefig-does-not-save-axes\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = 16\n",
    "\n",
    "versions = ['assumehrf','fithrf','fithrf_glmdenoise','fithrf_glmdenoise_rr']\n",
    "colors = ['dodgerblue','orange','limegreen','tomato']\n",
    "\n",
    "bins  = np.round(np.arange(-0.2,0.6,0.025),3)\n",
    "\n",
    "method = 'mean'\n",
    "min_nvox = 100\n",
    "\n",
    "allsubj_diffs = dict()\n",
    "\n",
    "# iterate through datasets\n",
    "for ds in ['v1']:\n",
    "    \n",
    "    allsubj_diffs[ds] = dict()\n",
    "    \n",
    "    for version in versions:\n",
    "        allsubj_diffs[ds][version] = []\n",
    "\n",
    "    # iterate through subjects\n",
    "    for subj in info[ds]['subjs']:\n",
    "                \n",
    "        plt.figure(figsize=(6,6))\n",
    "        \n",
    "        subj_mean_metric = reliability[ds][subj]['mean']\n",
    "        \n",
    "        subj_version_diffs = []\n",
    "        \n",
    "        # iterate through versions\n",
    "        v=0\n",
    "        for version in versions:\n",
    "            \n",
    "            this_version = reliability[ds][subj][version]\n",
    "            this_version_diffs = []\n",
    "            this_version_stds = []\n",
    "            for val in bins:\n",
    "           \n",
    "                validvox = subj_mean_metric > val\n",
    "                \n",
    "                if np.sum(validvox) > min_nvox:\n",
    "                    if method == 'mean':\n",
    "                        this_version_diffs.append(np.nanmean(this_version[validvox] - subj_mean_metric[validvox]))\n",
    "                    elif method == 'median':\n",
    "                        this_version_diffs.append(np.nanmedian(this_version[validvox] - subj_mean_metric[validvox]))\n",
    "                    this_version_stds.append(np.nanstd(this_version[validvox] - subj_mean_metric[validvox]))\n",
    "                else:\n",
    "                    this_version_diffs.append(np.nan)\n",
    "                    this_version_stds.append(np.nan)\n",
    "                    \n",
    "            \n",
    "            subj_version_diffs.append(this_version_diffs)\n",
    "            plt.plot(this_version_diffs,linewidth=12.5,color = colors[v],alpha=.9)\n",
    "            \n",
    "            v+=1\n",
    "            \n",
    "            allsubj_diffs[ds][version].append(this_version_diffs)\n",
    "                \n",
    "            \n",
    "        plt.plot(np.zeros((len(bins),)),'k--',linewidth=5)\n",
    "        plt.xticks(np.arange(len(bins))[::8],np.array(bins[::8]),fontsize=ft);\n",
    "        \n",
    "        plt.yticks(np.round(np.arange(-0.2,0.3,0.1),2),np.round(np.arange(-0.2,0.3,0.1),2),fontsize=33)\n",
    "        plt.ylim([-0.22,0.22])\n",
    "      \n",
    "        plt.xticks(fontsize=33)\n",
    "        plt.yticks(fontsize=33)\n",
    "        plt.grid('on')         \n",
    "        # get rid of the frame\n",
    "        for spine in plt.gca().spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        plt.xlabel('voxel reliability threshold')\n",
    "        plt.ylabel('difference from mean reliability (based on all versions)')\n",
    "        plt.legend(versions)\n",
    "        plt.savefig(f\"{figuredir_glmsingle}/reliability_lineplot.jpg\", bbox_inches='tight', dpi=150)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python (fmri)",
   "language": "python",
   "name": "fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
